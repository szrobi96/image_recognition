{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szrobi96/image_recognition/blob/main/Train_YOLO_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sUfcA8ZgR2t"
      },
      "source": [
        "# Train YOLO Models in Google Colab\n",
        "**Original Author:** Evan Juras, [EJ Technology Consultants](https://ejtech.io)\n",
        "\n",
        "**Adapted by:** RÃ³bert SzabÃ³, [email](mailto:szabo.robert96@hotmail.com)\n",
        "\n",
        "**Last updated:** July 08, 2025\n",
        "\n",
        "**Original GitHub:** [Train and Deploy YOLO Models (Candies)](https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models)\n",
        "\n",
        "**Adapted GitHub:** [Train and Deploy YOLO Models (Pill or Capsule)](https://github.com/szrobi96/image_recognition.git)\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This notebook uses [Ultralytics](https://docs.ultralytics.com/) to train YOLO11, YOLOv8, or YOLOv5 object detection models with a custom dataset. At the end of this Colab, you'll have a custom YOLO model that you can run on your PC, phone, or edge device like the Raspberry Pi.\n",
        "\n",
        "Original YouTube video that walks through this guide step by step.\n",
        "\n",
        "<p align=center>\n",
        "<a href=\"https://youtu.be/r0RspiLG260\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/Train_YOLO_Thumbnail2.png\" height=\"240\"><br>\n",
        "<i>Click here to go to the video!</i></a>\n",
        "</p>\n",
        "\n",
        "### Working in Colab\n",
        "Colab provides a virtual machine in your browser complete with a Linux OS, filesystem, Python environment, and best of all, a free GPU. We'll install PyTorch and Ultralytics in this environment and use it to train our model. Simply click the Play button on sections of code in this notebook to execute them on the virtual machine.\n",
        "\n",
        "### Navigation\n",
        "To navigate this notebook, use the table of contents in the left sidebar to jump from section to section.\n",
        "\n",
        "### Dataset\n",
        "This notebook uses a custom dataset of images of pills and capsules. The dataset is available on Roboflow https://universe.roboflow.com/seblful/pills-detection-s9ywn and can be directly downloaded in the desired format. The dataset is split into training and validation sets, with proper annotations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The easiest way to run python code on a PC is using Anaconda (or Miniconda). Anaconda sets up a virtual Python environment and allows you to easily install Python and all necessery tools. \n",
        "\n",
        "**1. Download and Install Anaconda**\n",
        "\n",
        "Go to the Anaconda download page at https://www.anaconda.com/download/success#:~:text=Miniconda%20Installers, and then download the package for your OS. When it's finished downloading, run the installer and click through the installation steps. You can use the default options for installation.\n",
        "\n",
        "**2. Set up virtual environment**\n",
        "\n",
        "Once it's installed, run Anaconda Prompt from the Start Bar. (If you're on macOS or Linux, just open a command terminal).\n",
        "\n",
        "Issue the following commands to create a new Python environment and activate it:\n",
        "\n",
        "```\n",
        "conda create --name yolo-env1 python=3.12 -y\n",
        "conda activate yolo-env1\n",
        "```\n",
        "\n",
        "Install Ultralytics (which also installs import libraries like OpenCV-Python, Numpy, and PyTorch) by issuing the following command:\n",
        "\n",
        "```\n",
        "pip install ultralytics\n",
        "```\n",
        "\n",
        "If you have an NVIDIA GPU, you can install the GPU-enabled version of PyTorch by issuing the following command:\n",
        "\n",
        "```\n",
        "pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "```\n",
        "\n",
        "**3. Download and run yolo_detect.py**\n",
        "\n",
        "Create a new folder called `my_model` in your home directory (or wherever you want to store your model files). You can do this by running:\n",
        "\n",
        "```\n",
        "mkdir my_model\n",
        "```\n",
        "Then, change into the `my_model` directory:\n",
        "\n",
        "```\n",
        "cd my_model\n",
        "```\n",
        "\n",
        "Download the `yolo_detect.py` script into the `my_model` folder using:\n",
        "\n",
        "```\n",
        "curl -o yolo_detect.py https://raw.githubusercontent.com/szrobi96/image_recognition/refs/heads/main/yolo_detect.py\n",
        "```\n",
        "\n",
        "Ultralytics provides already trained models that you can use for object detection. For example, you can use `yolo11n.pt`, which is a small, fast model that can detect 80 different classes of objects on a Webcam by issuing the following command:\n",
        "```\n",
        "python yolo_detect.py --model yolo11n.pt --source usb0 --resolution 1280x720\n",
        "```\n",
        "\n",
        "A window will appear showing a live feed from your webcam with boxes drawn around detected objects in each frame.\n",
        "\n",
        "An overview of stock models can be found in the [Ultralytics Models documentation](https://docs.ultralytics.com/models/). \n",
        "\n",
        "You can also run the model on an video file, image, or folder of images. To see a full list of arguments for `yolo_detect.py`, issue `python yolo_detect.py --help` or see the [README file](https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/blob/main/README.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train YOLO Models in Google Colab with Custom Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NW7LLv_QPOO"
      },
      "source": [
        "**Verify NVIDIA GPU Availability**\n",
        "\n",
        "Make sure you're using a GPU-equipped machine by going to \"Runtime\" -> \"Change runtime type\" in the top menu bar, and then selecting one of the GPU options in the Hardware accelerator section. Click Play on the following code block to verify that the NVIDIA GPU is present and ready for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfaWho47RGDf",
        "outputId": "48d9e92e-5cef-4f8a-c156-13d1d90060b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jul  7 11:28:34 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIHu25pnjjJ1"
      },
      "source": [
        "# 1.&nbsp;Gather and Label Training Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6Y1vBiRjpcq"
      },
      "source": [
        "Before we start training, we need to gather and label images that will be used for training the object detection model. A good starting point for a proof-of-concept model is 200 images. The training images should have random objects in the image along with the desired objects, and should have a variety of backgrounds and lighting conditions.\n",
        "\n",
        "There are a couple options for gathering images:\n",
        "\n",
        "\n",
        "*   Build a custom dataset by taking your own pictures of the objects and labeling them (this typically results in the best performance)\n",
        "*   Find a pre-made dataset from sources like [Roboflow Universe](), [Kaggle](), or [Google Images V7]() <-- ***This is our preferred option for this notebook, as it allows you to quickly get started with a pre-made dataset.***\n",
        "\n",
        "If you want to build your own dataset, there are several tools available for labeling images. One good option is [Label Studio](https://labelstud.io/?utm_source=youtube&utm_medium=video&utm_campaign=edjeelectronics), a free and open-source labeling tool that has a simple workflow while providing capabilities for more advanced features. My YouTube video that walks through this notebook (link to be added soon) shows how to label images with Label Studio.\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/label-studio-example.PNG\" height=\"380\"><br>\n",
        "<i>Example of a candy image labeled with Label Studio.</i>\n",
        "</p>\n",
        "\n",
        "* If you used Label Studio to label and export the images, they'll be exported in a `project.zip` file that contains the following:\n",
        "\n",
        "- An `images` folder containing the images\n",
        "- A `labels` folder containing the labels in YOLO annotation format\n",
        "- A `classes.txt` labelmap file that contains all the classes\n",
        "- A `notes.json` file that contains info specific to Label Studio (this file can be ignored)\n",
        "\n",
        "## Download the Pill & Capsule Dataset from Roboflow\n",
        "\n",
        "To train your model, you'll need a dataset of labeled images. The pill and capsule dataset is available for free on Roboflow Universe. Follow these steps to download it:\n",
        "\n",
        "1. Go to the [Pills Detection Dataset on Roboflow Universe](https://universe.roboflow.com/seblful/pills-detection-s9ywn).\n",
        "2. Click the **Dataset** button in the side panel.\n",
        "3. Click **Download Dataset** and then again **Download Dataset** in the popup. Click **Continue**.\n",
        "4. Under **Image and Annotation Format**, select your desired format (for this notebook, choose **YOLOv11**), check **Download zip to computer**, and click **Continue**.\n",
        "\n",
        "A `.zip` file containing your dataset will be downloaded to your computer.\n",
        "\n",
        "> **Tip:** If you have trouble downloading with Mozilla Firefox, try using Google Chrome or Microsoft Edge instead.\n",
        "\n",
        "Once your dataset is downloaded, you're ready to move on to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eDhuvzDfIFS"
      },
      "source": [
        "# 2.&nbsp;Upload Image Dataset and Prepare Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW_0c110fOiz"
      },
      "source": [
        "Next, we'll upload our dataset and prepare it for training with YOLO. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwKAqFIQSBpn"
      },
      "source": [
        "## 2.1 Upload images\n",
        "\n",
        "First, we need to upload the dataset to Colab. Here are a few options for moving the `.zip` folder into this Colab instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPZEM27IOh79"
      },
      "source": [
        "**Option 1. Upload through Google Colab**\n",
        "\n",
        "Upload the `*.zip` file to the Google Colab instance by clicking the \"Files\" icon on the left hand side of the browser, and then the \"Upload to session storage\" icon. Select the zip folder to upload it.\n",
        "\n",
        "<p>\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/doc/upload-colab-files.png\" height=\"240\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC4bZM1UWRdY"
      },
      "source": [
        "\n",
        "**Option 2. Copy from Google Drive**\n",
        "\n",
        "You can also upload your images to your personal Google Drive, mount the drive on this Colab session, and copy them over to the Colab filesystem. This option works well if you want to upload the images beforehand so you don't have to wait for them to upload each time you restart this Colab. If you have more than 50MB worth of images, I recommend using this option.\n",
        "\n",
        "First, upload the `*.zip` file to your Google Drive, and make note of the folder you uploaded them to. Replace `MyDrive/path/to/*.zip` with the path to your zip file. (For example, I uploaded the zip file to folder called \"pharma-dataset1\", so I would use `MyDrive/pharma-dataset1/*.zip` for the path). Then, run the following block of code to mount your Google Drive to this Colab session and copy the folder to this filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfQBSwDdWoWp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp /content/gdrive/MyDrive/path/to/data.zip /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Iz9eBzW5zm"
      },
      "source": [
        "## 2.2 Split images into train and validation folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58JuFGc2PatU"
      },
      "source": [
        "At this point, whether you used Option 1, or 2, you should be able to click the folder icon on the left and see your `data.zip` file in the list of files. Next, we'll unzip `data.zip` and create some folders to hold the images. Run the following code block to unzip the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8O6z-wVcPEF"
      },
      "outputs": [],
      "source": [
        "# Unzip images to a custom data folder\n",
        "!unzip -q /content/data.zip -d /content/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoPjqW6AYebn"
      },
      "source": [
        "Ultralytics requires a particular folder structure to store training data for models. The root folder is named â€œdataâ€. Inside, there are two main folders:\n",
        "\n",
        "*   **Train**: These are the actual images used to train the model. In one epoch of training, every image in the train set is passed into the neural network. The training algorithm adjusts the network weights to fit the data in the images.\n",
        "\n",
        "*   **Validation**: These images are used to check the model's performance at the end of each training epoch.\n",
        "\n",
        "... and a third, optional folder:\n",
        "\n",
        "*   **Test**: These images are used to test the model's performance after training is complete. This set is not used during training, but is used to evaluate the model's accuracy on unseen data.\n",
        "\n",
        "In each of these folders is a â€œimagesâ€ folder and a â€œlabelsâ€ folder, which hold the image files and annotation files respectively. The annotation files are in the YOLO format, which is a text file with the same name as the image file, but with a `.txt` extension. Each line in the text file contains the class ID and bounding box coordinates for an object in the image.\n",
        "\n",
        "The data is already in the correct format, and split into train and validation sets, so we can simply continue with the next step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2L2qGCJzwY9"
      },
      "source": [
        "# 3.&nbsp;Install Requirements (Ultralytics)\n",
        "\n",
        "Next, we'll install the Ultralytics library in this Google Colab instance. This Python library will be used to train the YOLO model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMEDk5byzxY5",
        "outputId": "aa56d2a8-aa68-45e3-e87d-e2a5b0537784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.162-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.162-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.162 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuZoMkSFN9XG"
      },
      "source": [
        "# 4.&nbsp;Configure Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5Kdh0GmQHS"
      },
      "source": [
        "There's one last step before we can run training: we need to create the Ultralytics training configuration YAML file. This file specifies the location of your train and validation data, and it also defines the model's classes. Since we downloaded the dataset from Roboflow, it should already be present. If you assembled the dataset another way, you may have to manually create the `data.yaml` configuration file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4letvP7X12ji"
      },
      "outputs": [],
      "source": [
        "!cat /content/data/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myP80_bnTNMi"
      },
      "source": [
        "# 5.&nbsp;Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfKspYasCzC8"
      },
      "source": [
        "## 5.1 Training Parameters\n",
        "Now that the data is organized and the config file is created, we're ready to start training! First, there are a few important parameters to decide on. Visit my article on [Training YOLO Models Locally](https://www.ejtech.io/learn/train-yolo-models) to learn more about these parameters and how to choose them.\n",
        "\n",
        "**Model architecture & size (`model`):**\n",
        "\n",
        "There are several YOLO11 models sizes available to train, including `yolo11n.pt`, `yolo11s.pt`, `yolo11m.pt`, `yolo11l.pt`, and `yolo11xl.pt`. Larger models run slower but have higher accuracy, while smaller models run faster but have lower accuracy. I made a brief YouTube video that compares performance of different YOLO models on a Raspberry Pi 5 and a laptop with a RTX 4050 GPU, [check it out here to get a sense of their speed accuracy](https://youtu.be/_WKS4E9SmkA). If you aren't sure which model size to use, `yolo11s.pt` is a good starting point.\n",
        "\n",
        "You can also train YOLOv8 or YOLOv5 models by substituting `yolo11` for `yolov8` or `yolov5`.\n",
        "\n",
        "\n",
        "**Number of epochs (`epochs`)**\n",
        "\n",
        "In machine learning, one â€œepochâ€ is one single pass through the full training dataset. Setting the number of epochs dictates how long the model will train for. The best amount of epochs to use depends on the size of the dataset and the model architecture. If your dataset has less than 200 images, a good starting point is 60 epochs. If your dataset has more than 200 images, a good starting point is 40 epochs.\n",
        "\n",
        "\n",
        "**Resolution (`imgsz`)**\n",
        "\n",
        "Resolution has a large impact on the speed and accuracy of the model: a lower resolution model will have higher speed but less accuracy. YOLO models are typically trained and inferenced at a 640x640 resolution. However, if you want your model to run faster or know you will be working with low-resolution images, try using a lower resolution like 480x480.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V17UjYU5ZQdR"
      },
      "source": [
        "## 5.2 Run Training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQi_hXnUVPr-"
      },
      "source": [
        "Run the following code block to begin training. If you want to use a different model, number of epochs, or resolution, change `model`, `epochs`, or `imgsz`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bbpob1gTPlo",
        "outputId": "8c089975-c4f4-4983-e757-61b057c6db93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.162 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/data/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=60, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
            "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
            " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
            " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 23        [16, 19, 22]  1    820182  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
            "YOLO11s summary: 181 layers, 9,428,566 parameters, 9,428,550 gradients, 21.6 GFLOPs\n",
            "\n",
            "Transferred 493/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2291.6Â±1165.7 MB/s, size: 402.5 KB)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data/train/labels.cache... 483 images, 1 backgrounds, 0 corrupt: 100% 483/483 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.1 ms, read: 403.9Â±103.6 MB/s, size: 43.9 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data/valid/labels.cache... 135 images, 0 backgrounds, 0 corrupt: 100% 135/135 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 60 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/60        12G       1.64      2.158      1.538         34       1024: 100% 31/31 [00:24<00:00,  1.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:13<00:00,  2.66s/it]\n",
            "                   all        135       2602      0.655      0.658      0.685       0.39\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/60      12.1G      1.456      1.264      1.378         69       1024: 100% 31/31 [00:21<00:00,  1.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:04<00:00,  1.14it/s]\n",
            "                   all        135       2602      0.464      0.646      0.475      0.239\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/60      13.9G      1.497      1.171      1.378         60       1024: 100% 31/31 [00:23<00:00,  1.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.67it/s]\n",
            "                   all        135       2602      0.394      0.473      0.404      0.177\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/60      12.1G       1.47      1.209      1.391         92       1024: 100% 31/31 [00:22<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:03<00:00,  1.53it/s]\n",
            "                   all        135       2602      0.649      0.644      0.635      0.325\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/60      11.6G      1.449      1.087      1.368         60       1024: 100% 31/31 [00:22<00:00,  1.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:03<00:00,  1.67it/s]\n",
            "                   all        135       2602      0.532      0.493      0.449      0.225\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/60      11.7G      1.447      1.107       1.37        108       1024: 100% 31/31 [00:22<00:00,  1.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.78it/s]\n",
            "                   all        135       2602      0.455      0.433       0.37      0.168\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/60      12.3G       1.39      1.013      1.335         72       1024: 100% 31/31 [00:24<00:00,  1.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.87it/s]\n",
            "                   all        135       2602      0.695      0.675      0.711      0.397\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/60      12.2G      1.369     0.9732       1.33        192       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.91it/s]\n",
            "                   all        135       2602      0.729      0.721       0.75      0.436\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/60      12.2G      1.432     0.9802      1.359        117       1024: 100% 31/31 [00:22<00:00,  1.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.79it/s]\n",
            "                   all        135       2602      0.762      0.753      0.793       0.46\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/60        12G      1.391      1.086      1.366         19       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.91it/s]\n",
            "                   all        135       2602      0.781      0.709      0.794      0.475\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/60      11.8G      1.351     0.9777      1.321         30       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.70it/s]\n",
            "                   all        135       2602      0.759      0.733      0.795      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/60      11.9G       1.37     0.9551      1.291        113       1024: 100% 31/31 [00:23<00:00,  1.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:03<00:00,  1.45it/s]\n",
            "                   all        135       2602       0.76      0.735      0.806      0.458\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/60        11G      1.342     0.9137      1.317        112       1024: 100% 31/31 [00:21<00:00,  1.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:03<00:00,  1.53it/s]\n",
            "                   all        135       2602      0.756      0.791      0.799      0.466\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/60      11.8G      1.362     0.8924       1.31        121       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:03<00:00,  1.51it/s]\n",
            "                   all        135       2602       0.81      0.775       0.84      0.498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/60      12.8G       1.35     0.9001      1.311        188       1024: 100% 31/31 [00:22<00:00,  1.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:03<00:00,  1.34it/s]\n",
            "                   all        135       2602       0.81       0.75      0.829      0.477\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/60      11.5G       1.35     0.8965      1.316        195       1024: 100% 31/31 [00:22<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.91it/s]\n",
            "                   all        135       2602      0.765      0.752      0.853       0.51\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/60      11.9G      1.333     0.8642      1.309         83       1024: 100% 31/31 [00:22<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.74it/s]\n",
            "                   all        135       2602      0.783      0.803      0.849      0.505\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/60      12.2G      1.294     0.8221       1.29         45       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.89it/s]\n",
            "                   all        135       2602      0.846      0.817       0.89      0.534\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/60      12.6G      1.285     0.8231       1.27        169       1024: 100% 31/31 [00:22<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.71it/s]\n",
            "                   all        135       2602      0.818      0.802      0.854      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/60      12.4G      1.328     0.8315      1.293         84       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.77it/s]\n",
            "                   all        135       2602      0.807        0.8      0.856      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/60      12.1G      1.306      0.804      1.295         74       1024: 100% 31/31 [00:22<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.73it/s]\n",
            "                   all        135       2602      0.816      0.815      0.864      0.503\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/60      11.8G      1.308     0.8256      1.307        178       1024: 100% 31/31 [00:22<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.85it/s]\n",
            "                   all        135       2602      0.816      0.786      0.875      0.517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/60      11.9G      1.277     0.7732      1.262         85       1024: 100% 31/31 [00:22<00:00,  1.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.88it/s]\n",
            "                   all        135       2602      0.845      0.822      0.893      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/60        12G      1.274     0.7623      1.252        193       1024: 100% 31/31 [00:23<00:00,  1.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.69it/s]\n",
            "                   all        135       2602      0.836      0.812      0.867      0.525\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/60      12.2G      1.237     0.7576      1.247        103       1024: 100% 31/31 [00:22<00:00,  1.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.94it/s]\n",
            "                   all        135       2602      0.841      0.848        0.9      0.551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/60      13.1G      1.294     0.7636       1.27         39       1024: 100% 31/31 [00:22<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.85it/s]\n",
            "                   all        135       2602      0.848      0.829        0.9      0.549\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/60        12G      1.222     0.7299       1.24         96       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.94it/s]\n",
            "                   all        135       2602      0.847       0.83      0.904      0.539\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/60      11.9G      1.223     0.7285      1.238         46       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "                   all        135       2602      0.886      0.848       0.92      0.552\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/60      11.9G      1.246     0.7158      1.247        100       1024: 100% 31/31 [00:22<00:00,  1.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "                   all        135       2602      0.869      0.827      0.902      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      30/60      12.1G      1.256     0.7623      1.256        322       1024: 100% 31/31 [00:22<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.01it/s]\n",
            "                   all        135       2602      0.797      0.795      0.855      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      31/60      11.5G      1.228     0.7255      1.249         73       1024: 100% 31/31 [00:22<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.99it/s]\n",
            "                   all        135       2602      0.871      0.844      0.911      0.537\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      32/60      12.3G      1.237     0.7213      1.237         76       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.89it/s]\n",
            "                   all        135       2602      0.869      0.835      0.908      0.559\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      33/60      12.2G      1.226     0.6938      1.244        228       1024: 100% 31/31 [00:23<00:00,  1.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.77it/s]\n",
            "                   all        135       2602      0.849      0.834      0.907      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      34/60      12.1G      1.209     0.6544      1.218        135       1024: 100% 31/31 [00:22<00:00,  1.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.96it/s]\n",
            "                   all        135       2602      0.859      0.841      0.913      0.567\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      35/60      11.7G      1.203     0.6818      1.234        123       1024: 100% 31/31 [00:22<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "                   all        135       2602      0.898      0.859      0.931      0.571\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      36/60      13.1G      1.179     0.6679      1.211         35       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.95it/s]\n",
            "                   all        135       2602      0.875      0.867      0.926       0.56\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      37/60        12G      1.177     0.6487      1.217         52       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "                   all        135       2602      0.867       0.86      0.916      0.561\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      38/60      11.7G      1.198     0.6461      1.208         62       1024: 100% 31/31 [00:22<00:00,  1.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.95it/s]\n",
            "                   all        135       2602      0.873      0.863      0.926       0.56\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      39/60        13G      1.181     0.6403      1.214         55       1024: 100% 31/31 [00:22<00:00,  1.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.96it/s]\n",
            "                   all        135       2602      0.902      0.848      0.928      0.564\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      40/60      12.4G      1.168     0.6338      1.204        140       1024: 100% 31/31 [00:22<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.00it/s]\n",
            "                   all        135       2602       0.87      0.881      0.932      0.583\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      41/60        13G      1.149     0.6067      1.174        161       1024: 100% 31/31 [00:22<00:00,  1.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.87it/s]\n",
            "                   all        135       2602      0.857      0.871      0.933      0.571\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      42/60      12.2G      1.152     0.6431      1.202         44       1024: 100% 31/31 [00:23<00:00,  1.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.88it/s]\n",
            "                   all        135       2602      0.884      0.875      0.934      0.576\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      43/60      12.4G      1.146     0.6134        1.2         62       1024: 100% 31/31 [00:22<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.94it/s]\n",
            "                   all        135       2602      0.874      0.868      0.933      0.579\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      44/60      12.2G      1.135     0.5937      1.177        156       1024: 100% 31/31 [00:22<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.02it/s]\n",
            "                   all        135       2602      0.895      0.872      0.935      0.577\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      45/60      12.5G      1.138     0.6074      1.193         54       1024: 100% 31/31 [00:22<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.01it/s]\n",
            "                   all        135       2602      0.854      0.872      0.905      0.561\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      46/60      12.1G      1.111     0.5847      1.174         50       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.00it/s]\n",
            "                   all        135       2602      0.866      0.855      0.917      0.574\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      47/60      11.8G      1.105      0.595      1.175         91       1024: 100% 31/31 [00:22<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.96it/s]\n",
            "                   all        135       2602       0.88      0.858      0.929      0.565\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      48/60      12.5G      1.131     0.5897      1.179        203       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.01it/s]\n",
            "                   all        135       2602      0.891      0.879      0.936      0.585\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      49/60      11.7G      1.123     0.6018      1.181         85       1024: 100% 31/31 [00:22<00:00,  1.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.99it/s]\n",
            "                   all        135       2602      0.871      0.876      0.926       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      50/60      11.8G      1.076     0.5411      1.147         91       1024: 100% 31/31 [00:22<00:00,  1.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "                   all        135       2602      0.856       0.87      0.926      0.578\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      51/60      11.2G        1.1     0.5432      1.189         54       1024: 100% 31/31 [00:25<00:00,  1.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.84it/s]\n",
            "                   all        135       2602      0.903       0.87      0.939      0.572\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      52/60      10.9G      1.085     0.5271      1.186         57       1024: 100% 31/31 [00:21<00:00,  1.46it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.89it/s]\n",
            "                   all        135       2602      0.889      0.861      0.936      0.589\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      53/60      11.3G      1.069     0.5228      1.169         80       1024: 100% 31/31 [00:21<00:00,  1.46it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.00it/s]\n",
            "                   all        135       2602      0.892      0.883      0.942      0.591\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      54/60      10.8G      1.063      0.499      1.172         76       1024: 100% 31/31 [00:21<00:00,  1.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.00it/s]\n",
            "                   all        135       2602      0.893      0.886      0.943       0.59\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      55/60      10.9G       1.06     0.5074      1.164         70       1024: 100% 31/31 [00:21<00:00,  1.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "                   all        135       2602      0.888      0.891      0.942      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      56/60      11.1G      1.033     0.4808      1.153         52       1024: 100% 31/31 [00:21<00:00,  1.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.93it/s]\n",
            "                   all        135       2602      0.885      0.891      0.943      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      57/60      11.6G      1.042     0.4864      1.159         88       1024: 100% 31/31 [00:21<00:00,  1.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.95it/s]\n",
            "                   all        135       2602      0.897      0.883      0.943      0.591\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      58/60      10.9G      1.041     0.4924      1.164         85       1024: 100% 31/31 [00:21<00:00,  1.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "                   all        135       2602      0.906       0.89      0.946      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      59/60      10.9G      1.033     0.4744      1.155         89       1024: 100% 31/31 [00:21<00:00,  1.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.89it/s]\n",
            "                   all        135       2602      0.895      0.891      0.947      0.591\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      60/60      11.2G      1.023     0.4868      1.164         10       1024: 100% 31/31 [00:22<00:00,  1.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.82it/s]\n",
            "                   all        135       2602      0.893      0.887      0.945      0.591\n",
            "\n",
            "60 epochs completed in 0.446 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 19.2MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 19.2MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics 8.3.162 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11s summary (fused): 100 layers, 9,413,574 parameters, 0 gradients, 21.3 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:06<00:00,  1.20s/it]\n",
            "                   all        135       2602      0.906       0.89      0.946      0.595\n",
            "              capsules         75        838      0.868      0.864      0.924      0.584\n",
            "               tablets         78       1764      0.945      0.917      0.969      0.607\n",
            "Speed: 0.8ms preprocess, 12.8ms inference, 0.0ms loss, 8.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n",
            "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "!yolo detect train data=/content/data/data.yaml model=yolo11s.pt epochs=10 imgsz=512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv0EYWJ5V6mC"
      },
      "source": [
        "The training algorithm will parse the images in the training and validation directories and then start training the model. At the end of each training epoch, the program runs the model on the validation dataset and reports the resulting mAP, precision, and recall. As training continues, the mAP should generally increase with each epoch. Training will end once it goes through the number of epochs specified by `epochs`.\n",
        "\n",
        "> **NOTE:** Make sure to allow training to run to completion, because an optimizer runs at the end of training that strips out unneeded layers from the model.\n",
        "\n",
        "The best trained model weights will be saved in `content/runs/detect/train/weights/best.pt`. Additional information about training is saved in the `content/runs/detect/train` folder, including a `results.png` file that shows how loss, precision, recall, and mAP progressed over each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo8BJRXeg0Ap"
      },
      "source": [
        "# 6.&nbsp;Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX3PTrEPacGY"
      },
      "source": [
        "The model has been trained; now it's time to test it! The commands below run the model on the images in the validation folder and then display the results for the first 10 images. This is a good way to confirm your model is working as expected. Click Play on the blocks below to see how your model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PooP5Vjsg2Jn"
      },
      "outputs": [],
      "source": [
        "!yolo detect predict model=runs/detect/train/weights/best.pt source=data/test/images save=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEEObQqoiGrs"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "for image_path in glob.glob(f'/content/runs/detect/predict/*.jpg')[:10]:\n",
        "  display(Image(filename=image_path, height=400))\n",
        "  print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGiQw_gWbSBa"
      },
      "source": [
        "The model should draw a box around each object of interest in each image. If it isn't doing a good job of detecting objects, here are a few tips:\n",
        "\n",
        "1. Double-check your dataset to make sure there are no labeling errors or conflicting examples.\n",
        "2. Increase the number of epochs used for training.\n",
        "3. Use a larger model size (e.g. `yolo11l.pt`).\n",
        "4. Add more images to the training dataset. See my [dataset video](https://www.youtube.com/watch?v=v0ssiOY6cfg) for tips on how to capture good training images and improve accuracy.\n",
        "\n",
        "You can also run the model on video files or other images images by uploading them to this notebook and using the above `!yolo detect predict` command, where `source` points to the location of the video file, image, or folder of images. The results will be saved in `runs/detect/predict`.\n",
        "\n",
        "Drawing boxes on images is great, but it isn't very useful in itself. It's also not very helpful to just run this models inside a Colab notebook: it's easier if we can just run it on a local computer. Continue to the next section to see how to download your newly trained model and run it on a local device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7yrFRViVczX"
      },
      "source": [
        "# 7.&nbsp;Deploy Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEtybPmB_ERi"
      },
      "source": [
        "Now that your custom model has been trained, it's ready to be downloaded and deployed in an application! YOLO models can run on a wide variety of hardware, including PCs, embedded systems, and phones. Ultralytics makes it easy to convert the YOLO models to various formats (`tflite`, `onnx`, etc.) and deploy them in a variety of environments.\n",
        "\n",
        "This section shows how to download the model and provides links to instructions for deploying it on your PC and edge devices like the Raspberry Pi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcoBAeHXa86W"
      },
      "source": [
        "## 7.1 Download YOLO Model\n",
        "\n",
        "First, zip and download the trained model by running the code blocks below.\n",
        "\n",
        "The code creates a folder named `my_model`, moves the model weights into it, and renames them from `best.pt` to `my_model.pt`. It also adds the training results in case you want to reference them later. It then zips the folder as `my_model.zip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcBdnOA9v85S"
      },
      "outputs": [],
      "source": [
        "# Create \"my_model\" folder to store model weights and train results\n",
        "!mkdir /content/my_model\n",
        "!cp /content/runs/detect/train/weights/best.pt /content/my_model/my_model.pt\n",
        "!cp -r /content/runs/detect/train /content/my_model\n",
        "\n",
        "# Zip into \"my_model.zip\"\n",
        "%cd my_model\n",
        "!zip /content/my_model.zip my_model.pt\n",
        "!zip -r /content/my_model.zip train\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ypwonynLVu"
      },
      "outputs": [],
      "source": [
        "# This takes forever for some reason, you can also just download the model from the sidebar\n",
        "from google.colab import files\n",
        "\n",
        "files.download('/content/my_model.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL06c6pb_UqZ"
      },
      "source": [
        "## 7.2 Deploy YOLO Model on Local Devices\n",
        "\n",
        "Next, we'll take our downloaded model and run it on a local device. This section provides instructions showing how to deploy YOLO models on various devices.\n",
        "\n",
        "I wrote a basic Python script, `yolo_detect.py`, that shows how to load a model, run inference on an image source, parse the inference results, and display boxes around each detected class in the image. The [script](https://github.com/szrobi96/image_recognition/blob/main/yolo_detect.py) gives an example of how to work with Ultralytics YOLO models in Python, and it can be used as a starting point for more advanced applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzaJQ2sGEPhP"
      },
      "source": [
        "### Deploy on PC (Windows, Linux, or macOS)\n",
        "\n",
        "We can run the model on a PC using the `yolo_detect.py` script just like we did in the first section of this notebook. Follow these steps to run the model on your PC:\n",
        "\n",
        "If you closed the Anaconda Prompt terminal, open it again and activate the `yolo-env1` environment by issuing:\n",
        "\n",
        "```shell\n",
        "conda activate yolo-env1\n",
        "```\n",
        "\n",
        "**1. Extract downloaded model**\n",
        "Take the `my_model.zip` file you downloaded in Step 7.1 and unzip it to a folder on your PC where you already have the `yolo_detect.py` script. You can use any unzip tool, such as the built-in Windows or macOS unzip tool, or a third-party tool like 7-Zip.\n",
        "\n",
        "**2. Change directory to the model folder**\n",
        "In the Anaconda Prompt, change to the directory where you unzipped the model. You can do this by using the `cd` command followed by the path to the folder where you unzipped the model. For example, if you unzipped it to a folder called `my_model` in your home directory, you would use:\n",
        "\n",
        "```shell\n",
        "cd path/to/folder/my_model\n",
        "```\n",
        "\n",
        "**3. Run the model on a webcam or video file**\n",
        "Now that you're in the model folder, you can run the `yolo_detect.py` script to run inference on a webcam or video file. The script takes several arguments, including the model file, source (webcam or video file), and resolution.\n",
        "\n",
        "To run the model on your webcam, use the following command:\n",
        "\n",
        "```shell\n",
        "python yolo_detect.py --model my_model.pt --source usb0 --resolution 1280x720\n",
        "```\n",
        "A window will appear showing a live feed from your webcam with boxes drawn around detected objects in each frame.\n",
        "\n",
        "If you want to run the model on a video file, replace `usb0` with the path to your video file. For example:\n",
        "\n",
        "```shell\n",
        "python yolo_detect.py --model my_model.pt --source path/to/video.mp4 --resolution 1280x720\n",
        "```\n",
        "\n",
        "You can also run the model on an video file, image, or folder of images. To see a full list of arguments for `yolo_detect.py`, issue `python yolo_detect.py --help` or see the [README file](https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/blob/main/README.md).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8fOJ4g8Q5x0"
      },
      "source": [
        "# 8.&nbsp;Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEZGuG1-Peg5"
      },
      "source": [
        "Congratulations! You've successfully trained and deployed a YOLO object detection model. ğŸ˜€\n",
        "\n",
        "Next, you can extend your application beyond just drawing boxes and counting objects. Add functionality like logging the number of objects detected over time or taking a picture when certain objects are detected.\n",
        "\n",
        "Thanks for working through this notebook, and good luck with your projects!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
